{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dv_processing in c:\\python311\\lib\\site-packages (1.7.9)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from dv_processing) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dv_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device discovery: found 0 devices.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import dv_processing as dv  # Import the dv_processing library for DAVIS 346 access\n",
    "\n",
    "cameras = dv.io.discoverDevices()\n",
    "\n",
    "print(f\"Device discovery: found {len(cameras)} devices.\")\n",
    "for camera_name in cameras:\n",
    "    print(f\"Detected device [{camera_name}]\")\n",
    "\n",
    "# Open any camera\n",
    "capture = dv.io.CameraCapture()\n",
    "\n",
    "# Print the camera name\n",
    "print(f\"Opened [{capture.getCameraName()}] camera, it provides:\")\n",
    "\n",
    "\n",
    "# Check whether event stream is available\n",
    "if capture.isEventStreamAvailable():\n",
    "    # Get the event stream resolution\n",
    "    resolution = capture.getEventResolution()\n",
    "\n",
    "    # Print the event stream capability with resolution value\n",
    "    print(f\"* Events at ({resolution.width}x{resolution.height}) resolution\")\n",
    "\n",
    "# Check whether frame stream is available\n",
    "if capture.isFrameStreamAvailable():\n",
    "    # Get the frame stream resolution\n",
    "    resolution = capture.getFrameResolution()\n",
    "\n",
    "    # Print the frame stream capability with resolution value\n",
    "    print(f\"* Frames at ({resolution.width}x{resolution.height}) resolution\")\n",
    "\n",
    "# Check whether the IMU stream is available\n",
    "if capture.isImuStreamAvailable():\n",
    "    # Print the imu data stream capability\n",
    "    print(\"* IMU measurements\")\n",
    "\n",
    "# Check whether the trigger stream is available\n",
    "if capture.isTriggerStreamAvailable():\n",
    "    # Print the trigger stream capability\n",
    "    print(\"* Triggers\")\n",
    "# Load a model\n",
    "modelpose = YOLO('yolov8n-pose.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 46\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boxing' 'jumping' 'running' 'sitting' 'squat' 'standing' 'walking']\n"
     ]
    }
   ],
   "source": [
    "# Define path to the main folder containing class subfolders\n",
    "data_path = \"C:\\\\Users\\\\ag701\\\\Desktop\\\\lstm\\\\LSTM\\\\MP_Data\"  # Replace with your actual path\n",
    "\n",
    "# Define actions based on subfolder names (modify as needed)\n",
    "actions = np.array(os.listdir(data_path))  # Get list of subfolders\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(15,34)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action83.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (126, 249, 255), (255, 166, 255),(16,117,245), (40, 166, 133)]\n",
    "def prob_viz(res, actions, input_frame, colors, keypoints):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    top_left_x = int(keypoints[0][:, 0].min().item())  # Minimum x-coordinate across all keypoints\n",
    "    top_left_y = int(keypoints[0][:, 1].min().item())-20  # Minimum y-coordinate across all keypoints\n",
    "    bottom_right_x = int(keypoints[0][:, 0].max().item())  # Maximum x-coordinate across all keypoints\n",
    "    bottom_right_y = int(keypoints[0][:, 1].max().item())+20 # Maximum y-coordinate across all keypoints\n",
    "\n",
    "    # Define bounding box thickness and color\n",
    "    thickness = 2\n",
    "    color = (0, 255, 0)  # Green for bounding box\n",
    "\n",
    "    # Draw the rectangle\n",
    "    cv2.rectangle(output_frame, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), color, thickness)\n",
    "\n",
    "    return output_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# import cv2\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# threshold = 0.6\n",
    "\n",
    "# cap = cv2.VideoCapture(\"Dataset/jumping/6032918-hd_1920_1080_25fps.mp4\")\n",
    "# # Set mediapipe model \n",
    "\n",
    "# while cap.isOpened():\n",
    "\n",
    "#     # Read feed\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         print(\"No frames left in video. Exiting...\")\n",
    "#         break\n",
    "    \n",
    "#     image = frame\n",
    "\n",
    "#     # Make detections\n",
    "#     results = modelpose.predict(frame)\n",
    "#     # Flatten keypoints\n",
    "#     # keypoints = np.array(results[0][0].keypoints.xy).flatten()\n",
    "#     for r in results:\n",
    "#         keypoints = np.array(r.keypoints.xy).flatten()  # Extract keypoints\n",
    "        \n",
    "    \n",
    "\n",
    "#     sequence.append(keypoints)\n",
    "#     sequence = sequence[-15:]\n",
    "    \n",
    "#     if len(sequence) == 15:\n",
    "#         res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#         print(actions[np.argmax(res)])\n",
    "        \n",
    "        \n",
    "#     #3. Viz logic\n",
    "#         if res[np.argmax(res)] > threshold: \n",
    "#             if len(sentence) > 0: \n",
    "#                 if actions[np.argmax(res)] != sentence[-1]:\n",
    "#                     sentence.append(actions[np.argmax(res)])\n",
    "#             else:\n",
    "#                 sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "#         if len(sentence) > 5: \n",
    "#             sentence = sentence[-5:]\n",
    "\n",
    "        \n",
    "#     cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#     cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "#     # Show to screen\n",
    "#     cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#     # Break gracefully\n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.6\n",
    "save_directory = \"CapturedFrames\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# cap = cv2.VideoCapture(\"C:\\\\Users\\\\ag701\\\\Desktop\\\\lstm\\\\LSTM\\\\jumping1.mp4\")\n",
    "\n",
    "\n",
    "# Use dv_processing for DAVIS 346 access\n",
    "cap = dv.io.CameraCapture()\n",
    "\n",
    "\n",
    "# Initiate a preview window\n",
    "cv2.namedWindow(\"Preview\", cv2.WINDOW_NORMAL)\n",
    "frame_counter = 0\n",
    "\n",
    "while cap.isRunning():\n",
    "    # Read feed\n",
    "    frame = cap.getNextFrame()\n",
    "\n",
    "    # if not ret:\n",
    "    #     print(\"No frames left in video. Exiting...\")\n",
    "    #     break\n",
    "    \n",
    "    if frame is None:\n",
    "        print(\"No frame received from DAVIS 346 camera.\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        image = frame\n",
    "\n",
    "        filename = f\"{save_directory}/frame_{frame_counter}.jpg\"\n",
    "        cv2.imwrite(filename, frame.image)\n",
    "        frame_counter += 1\n",
    "\n",
    "        # Make detections\n",
    "        results = modelpose.predict(filename)\n",
    "        \n",
    "        keyframes = []\n",
    "        # # Flatten keypoints\n",
    "        keypointsn = np.array(results[0][0].keypoints.xyn).flatten()\n",
    "        \n",
    "        if len(keypointsn) == 0:\n",
    "                keypointsn = torch.zeros(1, 17, 2)\n",
    "            \n",
    "        for r in results:\n",
    "            keypoints=[]\n",
    "            keypoints = r.keypoints.xy\n",
    "            print(\"keypoints\", keypoints.shape)\n",
    "            # if len(keypoints) > 0:  # Check for missing keypoints (optional)\n",
    "            #     keyframes.append(keypoints)\n",
    "            if len(keypoints) == 0:\n",
    "                keypoints = torch.zeros(1, 17, 2)\n",
    "        # if len(keyframes) > 0:  # Check for empty frames (optional)\n",
    "        #     keyframes = np.array(keyframes).flatten()\n",
    "        #     sequence.append(keyframes[-1])\n",
    "        #     sequence = sequence[-15:]\n",
    "\n",
    "        sequence.append(keypointsn)\n",
    "        sequence = sequence[-15:]\n",
    "    \n",
    "        if len(sequence) == 15:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors, keypoints)\n",
    "        \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
