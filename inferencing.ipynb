{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dv_processing\n",
      "  Obtaining dependency information for dv_processing from https://files.pythonhosted.org/packages/78/06/e6572ef771bf57102e807d7787a7c11d5b586b042dffd52e4ee761b883df/dv_processing-1.7.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached dv_processing-1.7.9-cp311-cp311-win_amd64.whl.metadata (238 bytes)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from dv_processing) (1.24.3)\n",
      "Using cached dv_processing-1.7.9-cp311-cp311-win_amd64.whl (16.0 MB)\n",
      "Installing collected packages: dv_processing\n",
      "Successfully installed dv_processing-1.7.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dv_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device discovery: found 0 devices.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import dv_processing as dv  # Import the dv_processing library for DAVIS 346 access\n",
    "\n",
    "cameras = dv.io.discoverDevices()\n",
    "\n",
    "print(f\"Device discovery: found {len(cameras)} devices.\")\n",
    "for camera_name in cameras:\n",
    "    print(f\"Detected device [{camera_name}]\")\n",
    "\n",
    "\n",
    "# Load a model\n",
    "modelpose = YOLO('yolov8n-pose.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 46\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boxing' 'jumping' 'running' 'sitting' 'squat' 'standing' 'walking']\n"
     ]
    }
   ],
   "source": [
    "# Define path to the main folder containing class subfolders\n",
    "data_path = \"C:\\\\Users\\\\ag701\\\\Desktop\\\\lstm\\\\LSTM\\\\MP_Data\"  # Replace with your actual path\n",
    "\n",
    "# Define actions based on subfolder names (modify as needed)\n",
    "actions = np.array(os.listdir(data_path))  # Get list of subfolders\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(15,34)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action83.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (126, 249, 255), (255, 166, 255),(16,117,245), (40, 166, 133)]\n",
    "def prob_viz(res, actions, input_frame, colors, keypoints):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    top_left_x = int(keypoints[0][:, 0].min().item())  # Minimum x-coordinate across all keypoints\n",
    "    top_left_y = int(keypoints[0][:, 1].min().item())-20  # Minimum y-coordinate across all keypoints\n",
    "    bottom_right_x = int(keypoints[0][:, 0].max().item())  # Maximum x-coordinate across all keypoints\n",
    "    bottom_right_y = int(keypoints[0][:, 1].max().item())+20 # Maximum y-coordinate across all keypoints\n",
    "\n",
    "    # Define bounding box thickness and color\n",
    "    thickness = 2\n",
    "    color = (0, 255, 0)  # Green for bounding box\n",
    "\n",
    "    # Draw the rectangle\n",
    "    cv2.rectangle(output_frame, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), color, thickness)\n",
    "\n",
    "    return output_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# import cv2\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# threshold = 0.6\n",
    "\n",
    "# cap = cv2.VideoCapture(\"Dataset/jumping/6032918-hd_1920_1080_25fps.mp4\")\n",
    "# # Set mediapipe model \n",
    "\n",
    "# while cap.isOpened():\n",
    "\n",
    "#     # Read feed\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         print(\"No frames left in video. Exiting...\")\n",
    "#         break\n",
    "    \n",
    "#     image = frame\n",
    "\n",
    "#     # Make detections\n",
    "#     results = modelpose.predict(frame)\n",
    "#     # Flatten keypoints\n",
    "#     # keypoints = np.array(results[0][0].keypoints.xy).flatten()\n",
    "#     for r in results:\n",
    "#         keypoints = np.array(r.keypoints.xy).flatten()  # Extract keypoints\n",
    "        \n",
    "    \n",
    "\n",
    "#     sequence.append(keypoints)\n",
    "#     sequence = sequence[-15:]\n",
    "    \n",
    "#     if len(sequence) == 15:\n",
    "#         res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#         print(actions[np.argmax(res)])\n",
    "        \n",
    "        \n",
    "#     #3. Viz logic\n",
    "#         if res[np.argmax(res)] > threshold: \n",
    "#             if len(sentence) > 0: \n",
    "#                 if actions[np.argmax(res)] != sentence[-1]:\n",
    "#                     sentence.append(actions[np.argmax(res)])\n",
    "#             else:\n",
    "#                 sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "#         if len(sentence) > 5: \n",
    "#             sentence = sentence[-5:]\n",
    "\n",
    "        \n",
    "#     cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#     cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "#     # Show to screen\n",
    "#     cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#     # Break gracefully\n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 2 persons, 68.3ms\n",
      "Speed: 9.9ms preprocess, 68.3ms inference, 12.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 persons, 92.1ms\n",
      "Speed: 2.9ms preprocess, 92.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([2, 17, 2])\n",
      "keypoints torch.Size([2, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 115.7ms\n",
      "Speed: 3.0ms preprocess, 115.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 97.2ms\n",
      "Speed: 0.0ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 98.0ms\n",
      "Speed: 1.0ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 90.5ms\n",
      "Speed: 3.0ms preprocess, 90.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 87.9ms\n",
      "Speed: 0.0ms preprocess, 87.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 80.8ms\n",
      "Speed: 0.0ms preprocess, 80.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 92.8ms\n",
      "Speed: 1.9ms preprocess, 92.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 83.4ms\n",
      "Speed: 0.0ms preprocess, 83.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 82.0ms\n",
      "Speed: 0.0ms preprocess, 82.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 91.8ms\n",
      "Speed: 0.0ms preprocess, 91.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 118.3ms\n",
      "Speed: 0.0ms preprocess, 118.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 person, 93.4ms\n",
      "Speed: 0.0ms preprocess, 93.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "keypoints torch.Size([1, 17, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 92.3ms\n",
      "Speed: 3.3ms preprocess, 92.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 80.1ms\n",
      "Speed: 3.0ms preprocess, 80.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n",
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 91.7ms\n",
      "Speed: 0.0ms preprocess, 91.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 84.4ms\n",
      "Speed: 0.0ms preprocess, 84.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 69.6ms\n",
      "Speed: 6.9ms preprocess, 69.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 76.9ms\n",
      "Speed: 0.0ms preprocess, 76.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "sitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 93.4ms\n",
      "Speed: 0.0ms preprocess, 93.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 79.8ms\n",
      "Speed: 0.0ms preprocess, 79.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 98.6ms\n",
      "Speed: 0.0ms preprocess, 98.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 86.1ms\n",
      "Speed: 0.0ms preprocess, 86.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 91.9ms\n",
      "Speed: 0.0ms preprocess, 91.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 93.0ms\n",
      "Speed: 0.0ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 81.9ms\n",
      "Speed: 0.0ms preprocess, 81.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 82.5ms\n",
      "Speed: 0.0ms preprocess, 82.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 82.5ms\n",
      "Speed: 0.0ms preprocess, 82.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 85.8ms\n",
      "Speed: 0.0ms preprocess, 85.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 87.2ms\n",
      "Speed: 5.8ms preprocess, 87.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 82.9ms\n",
      "Speed: 0.0ms preprocess, 82.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 87.1ms\n",
      "Speed: 0.0ms preprocess, 87.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 89.9ms\n",
      "Speed: 0.0ms preprocess, 89.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 2 persons, 80.1ms\n",
      "Speed: 0.0ms preprocess, 80.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([2, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 85.2ms\n",
      "Speed: 0.0ms preprocess, 85.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 90.7ms\n",
      "Speed: 0.0ms preprocess, 90.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 92.2ms\n",
      "Speed: 0.0ms preprocess, 92.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 99.9ms\n",
      "Speed: 0.0ms preprocess, 99.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 81.1ms\n",
      "Speed: 0.0ms preprocess, 81.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 78.7ms\n",
      "Speed: 0.0ms preprocess, 78.7ms inference, 10.2ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 106.0ms\n",
      "Speed: 0.0ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 102.5ms\n",
      "Speed: 0.0ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 83.4ms\n",
      "Speed: 0.0ms preprocess, 83.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 112.3ms\n",
      "Speed: 0.0ms preprocess, 112.3ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 92.6ms\n",
      "Speed: 0.0ms preprocess, 92.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 91.6ms\n",
      "Speed: 1.7ms preprocess, 91.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxing\n",
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 76.7ms\n",
      "Speed: 1.8ms preprocess, 76.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 80.0ms\n",
      "Speed: 0.0ms preprocess, 80.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 79.9ms\n",
      "Speed: 1.3ms preprocess, 79.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 70.0ms\n",
      "Speed: 5.7ms preprocess, 70.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 109.3ms\n",
      "Speed: 0.0ms preprocess, 109.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 85.9ms\n",
      "Speed: 3.4ms preprocess, 85.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 108.1ms\n",
      "Speed: 0.0ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "sitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 105.8ms\n",
      "Speed: 0.0ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 106.9ms\n",
      "Speed: 0.0ms preprocess, 106.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n",
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 80.6ms\n",
      "Speed: 2.5ms preprocess, 80.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 97.9ms\n",
      "Speed: 0.0ms preprocess, 97.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 94.2ms\n",
      "Speed: 8.4ms preprocess, 94.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n",
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 86.9ms\n",
      "Speed: 2.0ms preprocess, 86.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 77.5ms\n",
      "Speed: 0.0ms preprocess, 77.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 86.4ms\n",
      "Speed: 0.0ms preprocess, 86.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 103.0ms\n",
      "Speed: 0.0ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 94.1ms\n",
      "Speed: 0.0ms preprocess, 94.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x384 1 person, 80.8ms\n",
      "Speed: 0.0ms preprocess, 80.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 82.0ms\n",
      "Speed: 0.0ms preprocess, 82.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 91.7ms\n",
      "Speed: 3.4ms preprocess, 91.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "boxing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 94.4ms\n",
      "Speed: 0.9ms preprocess, 94.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints torch.Size([1, 17, 2])\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "boxing\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.6\n",
    "\n",
    "# cap = cv2.VideoCapture(\"C:\\\\Users\\\\ag701\\\\Desktop\\\\lstm\\\\LSTM\\\\jumping1.mp4\")\n",
    "# # Set mediapipe model \n",
    "\n",
    "\n",
    "# Use dv_processing for DAVIS 346 access\n",
    "cap = dv.io.CameraCapture()\n",
    "\n",
    "\n",
    "# Initiate a preview window\n",
    "cv2.namedWindow(\"Preview\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"No frames left in video. Exiting...\")\n",
    "        break\n",
    "    \n",
    "    image = frame\n",
    "\n",
    "    # Make detections\n",
    "    results = modelpose.predict(frame)\n",
    "    \n",
    "    keyframes = []\n",
    "    # # Flatten keypoints\n",
    "    keypointsn = np.array(results[0][0].keypoints.xyn).flatten()\n",
    "    \n",
    "    if len(keypointsn) == 0:\n",
    "            keypointsn = torch.zeros(1, 17, 2)\n",
    "            \n",
    "    for r in results:\n",
    "        keypoints=[]\n",
    "        keypoints = r.keypoints.xy\n",
    "        print(\"keypoints\", keypoints.shape)\n",
    "        # if len(keypoints) > 0:  # Check for missing keypoints (optional)\n",
    "        #     keyframes.append(keypoints)\n",
    "        if len(keypoints) == 0:\n",
    "            keypoints = torch.zeros(1, 17, 2)\n",
    "    # if len(keyframes) > 0:  # Check for empty frames (optional)\n",
    "    #     keyframes = np.array(keyframes).flatten()\n",
    "    #     sequence.append(keyframes[-1])\n",
    "    #     sequence = sequence[-15:]\n",
    "\n",
    "    sequence.append(keypointsn)\n",
    "    sequence = sequence[-15:]\n",
    "    \n",
    "    if len(sequence) == 15:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        print(actions[np.argmax(res)])\n",
    "        \n",
    "        \n",
    "    #3. Viz logic\n",
    "        if res[np.argmax(res)] > threshold: \n",
    "            if len(sentence) > 0: \n",
    "                if actions[np.argmax(res)] != sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 5: \n",
    "            sentence = sentence[-5:]\n",
    "\n",
    "        # Viz probabilities\n",
    "        image = prob_viz(res, actions, image, colors, keypoints)\n",
    "        \n",
    "    cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "    cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Show to screen\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "    # Break gracefully\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
